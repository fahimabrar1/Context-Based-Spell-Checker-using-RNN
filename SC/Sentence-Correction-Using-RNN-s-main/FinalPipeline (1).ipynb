{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalPipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5fpzwFsdW2"
      },
      "source": [
        "## Final Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCVvuZM9cKp"
      },
      "source": [
        "###**Encoder and Decoder with  Attention machanisam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O91z4s4X9ydE"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,vocab_size,embedding_size,lstm_size,input_length):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.enc_units=lstm_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_size\n",
        "        self.input_length = input_length\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, \n",
        "                                 input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\",weights=[Encoder_embedding_matrix])\n",
        "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "        embedding= self.embedding(input_sequence)\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c= self.lstm(embedding, initial_state = states)\n",
        "      \n",
        "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
        "   \n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      return tf.zeros((batch_size, self.enc_units)),tf.zeros((batch_size, self.enc_units))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utq-Dr3J9ylp"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.scoring_function=scoring_function\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      pass\n",
        "    if scoring_function == 'general':\n",
        "      self.dense = tf.keras.layers.Dense(att_units)\n",
        "\n",
        "      # Intialize variables needed for General score function here\n",
        "      pass\n",
        "    elif scoring_function == 'concat':\n",
        "\n",
        "      self.dense = tf.keras.layers.Dense(att_units, activation='tanh')\n",
        "      self.dense1 = tf.keras.layers.Dense(1)\n",
        "\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      pass\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "\n",
        "    if self.scoring_function == 'dot':\n",
        "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state, 1)\n",
        "        score = tf.matmul(decoder_hidden_state,encoder_output,transpose_b=True)\n",
        "        attention_weights = tf.keras.activations.softmax(score, axis=-1) \n",
        "        context_vector = tf.matmul(attention_weights, encoder_output)\n",
        "    \n",
        "        context_vector=tf.reduce_sum(context_vector, axis=1)\n",
        "      \n",
        "        attention_weights=tf.reduce_sum(attention_weights, axis=1)\n",
        "        attention_weights=tf.expand_dims(attention_weights, 1)\n",
        "    \n",
        "        return context_vector,attention_weights\n",
        "\n",
        "        # Implement Dot score function here\n",
        "        pass\n",
        "\n",
        "    elif self.scoring_function == 'general':\n",
        "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state, 1)\n",
        "\n",
        "        score = tf.matmul(decoder_hidden_state, self.dense(\n",
        "                encoder_output), transpose_b=True)\n",
        "      \n",
        "        attention_weights = tf.keras.activations.softmax(score, axis=-1) \n",
        "\n",
        "        context_vector = tf.matmul(attention_weights, encoder_output)\n",
        "      \n",
        "        context_vector=tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        attention_weights=tf.reduce_sum(attention_weights, axis=1)\n",
        "\n",
        "        attention_weights=tf.expand_dims(attention_weights, 1)\n",
        "    \n",
        "        return context_vector,attention_weights\n",
        "      # Implement General score function here\n",
        "        pass \n",
        "\n",
        "    elif self.scoring_function == 'concat':\n",
        "\n",
        "      decoder_hidden_state=tf.expand_dims(decoder_hidden_state, 1)\n",
        "    \n",
        "      decoder_hidden_state = tf.tile(\n",
        "                decoder_hidden_state, [1,50, 1])\n",
        "      \n",
        "      score = self.dense1(\n",
        "                self.dense(tf.concat((decoder_hidden_state, encoder_output), axis=-1)))\n",
        "       \n",
        "      score = tf.transpose(score, [0, 2, 1])\n",
        "        \n",
        "      attention_weights = tf.keras.activations.softmax(score, axis=-1) \n",
        "\n",
        "      context_vector = tf.matmul(attention_weights, encoder_output)\n",
        "      \n",
        "      context_vector=tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "      attention_weights=tf.reduce_sum(attention_weights, axis=1)\n",
        "\n",
        "      attention_weights=tf.expand_dims(attention_weights, 1)\n",
        "    \n",
        "      return context_vector,attention_weights\n",
        "\n",
        "      # Implement General score function here\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnQs8Oef9yoP"
      },
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "    \n",
        "      def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "            super(One_Step_Decoder, self).__init__()\n",
        "            self.dec_units=dec_units\n",
        "            self.vocab_size = tar_vocab_size\n",
        "            self.embedding_dim = embedding_dim\n",
        "            self.input_length = input_length\n",
        "            self.attention = Attention(score_fun, dec_units)\n",
        "            self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, \n",
        "                                   name=\"embedding_layer_encoder\",weights=[Decoder_embedding_matrix])\n",
        "\n",
        "            self.lstm = LSTM(self.dec_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "           # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "            self.DenseLayer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "\n",
        "      def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "        embedding= self.embedding(input_to_decoder)\n",
        "        \n",
        "        context_vector,attention_weights =self.attention(state_h,encoder_output)\n",
        "\n",
        "        context_vector=tf.expand_dims(context_vector, 1)\n",
        "        lstm_input = tf.concat(\n",
        "                [tf.squeeze(context_vector, 1), tf.squeeze(embedding, 1)], 1)\n",
        "      \n",
        "        states=[state_h,state_c]\n",
        "\n",
        "        lstm_input=tf.expand_dims(lstm_input, 1)\n",
        "\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c= self.lstm(lstm_input, initial_state = states)\n",
        "        \n",
        "        Output=self.DenseLayer(self.lstm_output)\n",
        "        \n",
        "        Output=tf.reduce_sum(Output, axis=1)\n",
        "\n",
        "        return Output,self.lstm_state_h,self.lstm_state_c,attention_weights,context_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIza6Q0S9yqc"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        self.out_vocab_size=out_vocab_size\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.input_length=input_length\n",
        "        self.dec_units=dec_units\n",
        "        self.score_fun=score_fun\n",
        "        self.att_units=att_units\n",
        "\n",
        "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "        self.onestepdecoder=One_Step_Decoder(self.out_vocab_size, self.embedding_dim,  self.input_length, self.dec_units ,self.score_fun,self.att_units)\n",
        "  \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "      \n",
        "        \n",
        "        all_outputs=tf.TensorArray(tf.float32,size=self.input_length,name=\"outputArray\")\n",
        "\n",
        "        for i in range(self.input_length):\n",
        "          decoder_input = tf.expand_dims(input_to_decoder[:, i], 1)\n",
        "          output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.onestepdecoder(decoder_input,encoder_output,decoder_hidden_state,decoder_cell_state)\n",
        "          \n",
        "          all_outputs=all_outputs.write(i,output)\n",
        "        \n",
        "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
        "        \n",
        "        return all_outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A7m82yB9ytd"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size,score_fun,attn_units,batch_size):\n",
        "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
        "        self.batch_size=batch_size\n",
        "        self.score_fun=score_fun\n",
        "        self.attn_units=attn_units\n",
        "        self.encoder = Encoder(vocab_size=vocab_size_normal_text+1, embedding_size=300,lstm_size=300, input_length=encoder_inputs_length)\n",
        "        self.decoder = Decoder(out_vocab_size=vocab_size_eng+1, embedding_dim=300, input_length=decoder_inputs_length,dec_units=300,score_fun=self.score_fun,att_units=self.attn_units)\n",
        "        \n",
        "               \n",
        "    def call(self, data):\n",
        "        input,output = data[0], data[1]\n",
        "    \n",
        "    \n",
        "        initial_state=self.encoder.initialize_states(self.batch_size)\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input,initial_state)\n",
        "        decoder_output                       = self.decoder(output,encoder_output, encoder_h, encoder_c)\n",
        "        return decoder_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RawCXC4hYsNm"
      },
      "source": [
        "**Loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISqHpGl49yv2"
      },
      "source": [
        "def lossfunction(y_true, y_pred): \n",
        "    \n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(y_true, y_pred, sample_weight=mask)\n",
        "  \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fyPJsjH_gYr"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "    pred_value= K.cast(K.argmax(y_pred, axis=-1), dtype='float32')\n",
        "    true_value = K.cast(K.equal(y_true, pred_value), dtype='float32')\n",
        "\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * true_value)\n",
        "    n_total = K.sum(mask)\n",
        "  \n",
        "    return n_correct / n_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh-9nOVj_n-L"
      },
      "source": [
        "import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping,TerminateOnNaN,TensorBoard\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1)\n",
        "filepath=\"/content/drive/MyDrive/CaseStudy2/Model5/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=4, save_best_only=True, mode='auto')\n",
        "logdir = \"/content/drive/MyDrive/CaseStudy2/Model5/Logs/fit_model2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_summary_writer = tf.summary.create_file_writer(logdir)\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir,histogram_freq=1,profile_batch = 100000000)\n",
        "callback_list = [checkpoint,tensorboard_callback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idaF6NJoAcvo",
        "outputId": "04a86973-7e24-4b4c-c889-d1e9adbc92a7"
      },
      "source": [
        "model  = MyModel(encoder_inputs_length=50,decoder_inputs_length=50,output_vocab_size=vocab_size_eng+1,score_fun='concat',attn_units=50,batch_size=50)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer,loss=lossfunction,metrics=[accuracy])\n",
        "train_steps=train.shape[0]//50\n",
        "valid_steps=validation.shape[0]//50\n",
        "\n",
        "Output=model.fit_generator(train_dataloader,steps_per_epoch=train_steps,epochs=25, validation_data=test_dataloader, validation_steps=valid_steps,callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "88/88 [==============================] - 94s 393ms/step - loss: 1.9371 - accuracy: 0.0520 - val_loss: 1.5102 - val_accuracy: 0.0696\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.06956, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-01-0.0696.hdf5\n",
            "Epoch 2/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 1.7003 - accuracy: 0.0633 - val_loss: 1.4361 - val_accuracy: 0.1137\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.06956 to 0.11371, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-02-0.1137.hdf5\n",
            "Epoch 3/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 1.5986 - accuracy: 0.1152 - val_loss: 1.3173 - val_accuracy: 0.1685\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.11371 to 0.16851, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-03-0.1685.hdf5\n",
            "Epoch 4/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 1.4864 - accuracy: 0.1620 - val_loss: 1.2309 - val_accuracy: 0.1960\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.16851 to 0.19602, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-04-0.1960.hdf5\n",
            "Epoch 5/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 1.3465 - accuracy: 0.1963 - val_loss: 1.1574 - val_accuracy: 0.2275\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.19602 to 0.22754, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-05-0.2275.hdf5\n",
            "Epoch 6/25\n",
            "88/88 [==============================] - 16s 178ms/step - loss: 1.2846 - accuracy: 0.2272 - val_loss: 1.0916 - val_accuracy: 0.2604\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.22754 to 0.26036, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-06-0.2604.hdf5\n",
            "Epoch 7/25\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 1.2007 - accuracy: 0.2648 - val_loss: 1.0218 - val_accuracy: 0.2915\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.26036 to 0.29153, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-07-0.2915.hdf5\n",
            "Epoch 8/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 1.0940 - accuracy: 0.3045 - val_loss: 0.9541 - val_accuracy: 0.3316\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.29153 to 0.33159, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-08-0.3316.hdf5\n",
            "Epoch 9/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 1.0029 - accuracy: 0.3564 - val_loss: 0.8869 - val_accuracy: 0.3726\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.33159 to 0.37260, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-09-0.3726.hdf5\n",
            "Epoch 10/25\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 0.8774 - accuracy: 0.4148 - val_loss: 0.8182 - val_accuracy: 0.4214\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.37260 to 0.42143, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-10-0.4214.hdf5\n",
            "Epoch 11/25\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 0.7784 - accuracy: 0.4732 - val_loss: 0.7555 - val_accuracy: 0.4683\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.42143 to 0.46834, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-11-0.4683.hdf5\n",
            "Epoch 12/25\n",
            "88/88 [==============================] - 16s 182ms/step - loss: 0.6690 - accuracy: 0.5411 - val_loss: 0.6734 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.46834 to 0.52856, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-12-0.5286.hdf5\n",
            "Epoch 13/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.5729 - accuracy: 0.6201 - val_loss: 0.6175 - val_accuracy: 0.5684\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.52856 to 0.56838, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-13-0.5684.hdf5\n",
            "Epoch 14/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.4929 - accuracy: 0.6771 - val_loss: 0.5652 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.56838 to 0.60940, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-14-0.6094.hdf5\n",
            "Epoch 15/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.4159 - accuracy: 0.7390 - val_loss: 0.5188 - val_accuracy: 0.6518\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.60940 to 0.65181, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-15-0.6518.hdf5\n",
            "Epoch 16/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.3403 - accuracy: 0.7966 - val_loss: 0.4875 - val_accuracy: 0.6731\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.65181 to 0.67310, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-16-0.6731.hdf5\n",
            "Epoch 17/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.2887 - accuracy: 0.8340 - val_loss: 0.4835 - val_accuracy: 0.6735\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.67310 to 0.67348, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-17-0.6735.hdf5\n",
            "Epoch 18/25\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 0.2540 - accuracy: 0.8587 - val_loss: 0.4438 - val_accuracy: 0.7088\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.67348 to 0.70884, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-18-0.7088.hdf5\n",
            "Epoch 19/25\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 0.2118 - accuracy: 0.8862 - val_loss: 0.4190 - val_accuracy: 0.7238\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.70884 to 0.72376, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-19-0.7238.hdf5\n",
            "Epoch 20/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 0.1747 - accuracy: 0.9102 - val_loss: 0.4233 - val_accuracy: 0.7268\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.72376 to 0.72681, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-20-0.7268.hdf5\n",
            "Epoch 21/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.1694 - accuracy: 0.9140 - val_loss: 0.4020 - val_accuracy: 0.7386\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.72681 to 0.73860, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-21-0.7386.hdf5\n",
            "Epoch 22/25\n",
            "88/88 [==============================] - 16s 180ms/step - loss: 0.1455 - accuracy: 0.9227 - val_loss: 0.3857 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.73860 to 0.75954, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-22-0.7595.hdf5\n",
            "Epoch 23/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 0.1256 - accuracy: 0.9358 - val_loss: 0.3733 - val_accuracy: 0.7691\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.75954 to 0.76909, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-23-0.7691.hdf5\n",
            "Epoch 24/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 0.1050 - accuracy: 0.9478 - val_loss: 0.3576 - val_accuracy: 0.7821\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.76909 to 0.78215, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-24-0.7821.hdf5\n",
            "Epoch 25/25\n",
            "88/88 [==============================] - 16s 179ms/step - loss: 0.0850 - accuracy: 0.9594 - val_loss: 0.3410 - val_accuracy: 0.7903\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.78215 to 0.79028, saving model to /content/drive/MyDrive/CaseStudy2/Model5_Att/weights-25-0.7903.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTWftRVtYG0Q"
      },
      "source": [
        "Probabilities=model.predict(validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7qWRtwcAvXw",
        "outputId": "bb5b9dec-1fcc-43cc-f56c-9e3213f830ab"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_4 (Encoder)          multiple                  4293000   \n",
            "_________________________________________________________________\n",
            "decoder_4 (Decoder)          multiple                  2965035   \n",
            "=================================================================\n",
            "Total params: 7,258,035\n",
            "Trainable params: 7,258,035\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBqfPdm7Y1sC"
      },
      "source": [
        "**Inference Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzvODeQY0jZE"
      },
      "source": [
        "##### **BeamSearch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfgP-EvH3GIf"
      },
      "source": [
        "# https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "  sequences = [[list(), 0.0]]\n",
        "  for row in data:\n",
        "    all_candidates = list()\n",
        "    for i in range(len(sequences)):\n",
        "      seq, score = sequences[i]\n",
        "      for j in range(len(row)):\n",
        "        try:\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "        except ValueError as e:\n",
        "          candidate = [seq + [j], 0]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # select k best\n",
        "    sequences = ordered[:k]\n",
        "  return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNtOT7Ty_oH-"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "def evaluate(sentence):\n",
        "\n",
        "    max_length_targ=10\n",
        "    max_length_inp=50\n",
        "\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess(sentence)\n",
        "    inputs = [tknizer_normal_text.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs =  tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    #hidden = [tf.zeros((1, 20))]\n",
        "    initial_state=model.layers[0].initialize_states(batch_size=1)\n",
        "    encoder_output, encoder_h, encoder_c = model.layers[0](inputs,initial_state) \n",
        "    dec_hidden = encoder_h\n",
        "    dec_cellstate= encoder_c\n",
        "    dec_input = tf.expand_dims([tknizer_english.word_index['start']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        Output, dec_hidden,dec_cellstate,attention_weights,context_vector = model.layers[1].onestepdecoder(dec_input, encoder_output,dec_hidden,dec_cellstate)\n",
        "        #Beam Search Decoder\n",
        "        Result_beam_list=beam_search_decoder(Output,k=1)\n",
        "        Result_beam=Result_beam_list[0][0]\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(Output[0]).numpy()\n",
        "        #Predicted ID using beam search decoder\n",
        "        result += tknizer_english.index_word[Result_beam[0]] + ' '\n",
        "        if tknizer_english.index_word[predicted_id] == 'end':\n",
        "            return result, sentence, attention_plot\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GiOxUoWY-g7"
      },
      "source": [
        "**Plotting Mechanisam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVpzLwc5AA0W"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)  \n",
        "    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZniI_rkDAA8r"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    print(\"-\"*50)\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '),    result.split(' '))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "PYm6LiiKn2os",
        "outputId": "5366285e-f225-4cc8-8b2b-08a817a17360"
      },
      "source": [
        "result=translate(\"haha ok one m going to make up late too\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: haha ok one m going to make up late too\n",
            "Predicted translation: haha ok i am going to make late too too \n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyGEQjZoBj2p"
      },
      "source": [
        "### Blue Score on validation data using Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UymvKKBU_Eq",
        "outputId": "a209d275-760e-454f-842f-5579e3849a76"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "def BleuScore(validation):\n",
        "    input=list(validation['NormalizedText'])\n",
        "    Y_true=list(validation['Original_English_Text_out'])\n",
        "    bleuscores=[]\n",
        "    for i in range(len(input)):\n",
        "        try:\n",
        "            result, sentence, attention_plot = evaluate(input[i])\n",
        "        except KeyError as e:\n",
        "            pass  \n",
        "    bleuscores.append(bleu.sentence_bleu(Y_true[i], result))\n",
        "    return sum(bleuscores)/len(bleuscores)\n",
        "\n",
        "AvearageScore=BleuScore(validation)\n",
        "\n",
        "print(\"Avearage Bleuscore for concat score function :\",AvearageScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avearage Bleuscore for concat score function : 0.6887246539984299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTe5wm-VBzlf"
      },
      "source": [
        "### Steps Performed:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2BuV8iAsFJ"
      },
      "source": [
        "1.Extracted scoial media text data(normal text data) and original english data from the given source file\n",
        "\n",
        "2.Used data agumentation using NLPAUG library and used two types of augmentation methods 1.Synonym Agumentation and 2.FastText agumentation for the words and generated nearly 4000 data points which then concatenated with source data points and overall data set size is 6000 points.\n",
        "\n",
        "3.Preprocessed data using re module and removed all puntuation marks and other special symbols.\n",
        "\n",
        "4.Created embedding weights using fastext model and used this weights in enbedding layer in neural network\n",
        "\n",
        "5. Using Data generators trained encoder and decoder model and get the train accuracy 85% and test accuracy 60%\n",
        "\n",
        "6. Trained one more neural network model using Bhendu attention mechanisam and it gives 95% train accuracy and 75% test accuracy.\n",
        "\n",
        "7. The Average Blue score of the neural network using beam search is 68% and it is pretty good"
      ]
    }
  ]
}